module AnalyticsDashboardGenerator

  # Gets information from a request index, which is either a flag (generate, recent, peek) or a number
  # Flags do certain actions.
    # ..."Generate" creates a new cache entry, saves it, and returns it (only called by resque worker)
    # ..."peek" creates a new cache entry, and returns it but doesn't save it
    # ..."recent" finds and returns the most recent cache entry
  # Index requests the cache at that index (starts at 1).

  # TO-DO: Yeah, this should probably be refactored so it makes sense...
  def self.dashboard_info index

    # The most recent cache is stored in the head.  If the index requested is greater than the head, that means
    # the user wants to peek (see the caches as they are, but not save them).  So we need to detect that and add the flag.
    if index != 'generate' and index != 'recent' and index != 'peek'
      index = index.to_i
      index = 'peek' if index > get_head
    end

    # If the user is generating or peeking, we need to do the Ruby to create the cache.
    # This is also called by a resque worker every fifteen minutes to populate the cache.
    if index == 'generate' || index == 'peek'
      loaninfo = get_loan_info                 # For each loan, collect data
      stats = get_current_stats                # Collect stats on loans
      historical_stats = get_historical_stats  # Get historical stats from R server

      loan_hash = {'loaninfo' => loaninfo, 'stats' => stats, 'historical_stats' => historical_stats, '_internal' => {'generated_at' => Time.now}}
      if index == 'generate'                                 # Only save the cache if we're generating
        # Use Rob K.'s method to store the cache in AWS S3
        new_head = get_head + 1
        DashMPI.store(new_head, "loaninfo_head")             # Update the head
        DashMPI.store(loan_hash, "loaninfo_#{new_head}")     # Save the information at that index
      end
      loan_hash

    elsif index == 'recent'
      DashMPI.read("loaninfo_" + get_head.to_s)              # Get the most recent cache (at head)
      
    else                                                     # If this is triggered with a number, get the information at that number
      DashMPI.read("loaninfo_#{index}")
    end
  end


  def self.get_head
    head = DashMPI.read("loaninfo_head")
    if head.nil?
      head = 0
      DashMPI.store(head, "loaninfo_head")
    end
    head
  end


  def self.get_loans
    @loans ||= Loan.includes(:customer_application).includes(customer: :credit_reports).limit(200).order('id desc').all
  end


  def self.get_loan_info
    get_loans
    active_model_ids = ModelSwitch.current(:all).values.compact.map(&:id)
    loaninfo = @loans.map do |loan|
      has_score =
        CreditModel::CreditModelScore.where(customer_id: loan.customer.id, credit_model_id: active_model_ids).exists?
      {
        'loan_id' => loan.id,
        'created' => loan.created_at.to_datetime.strftime("at %I:%M on %m-%d"),
        'score_missing' => !has_score,
        'letter_customer' => loan.customer_application.try(:transunion_letter?),
        'has_microbilt' => loan.customer.credit_reports.microbilt.exists?,
        'has_clarity' => loan.customer.credit_reports.clarity.exists?,
        'link' => Rails.application.routes.url_helpers.admin_loan_path(loan.id)
      }
    end
  end

  def self.get_top_20_vars 
    ENV['CREDIT_MODEL_SERVICE_URL'] = 'http://avant-analytics-dev.elasticbeanstalk.com'  # REMOVE ME
    @top_20_vars ||= ModelStats.get_top_x_vars_from_R_server(ModelStats.current_model_version, 20)
  end

  def self.get_historical_stats
    @historical_stats ||= ModelStats.get_variable_stats(ModelStats.current_model_version, get_top_20_vars)
  end

  def self.get_current_stats
    curr_model_object = ModelStats.current_model_object
    data = {}
    get_loans.each do |loan|
      d = (curr_model_object.data(loan) rescue {})
      d.delete_if { |k,v| get_top_20_vars.exclude?(k) }
      data[loan.id] = d
    end

    return_hash = {
      'missing_percentages' => tmp = get_historical_stats['means'].keys.map { |key| { key.to_s => 0 } }.reduce(:merge),
      'means' => tmp.deep_dup
    }
    data.each do |loan_id, within_loan_data|
      all_keys = return_hash['missing_percentages'].keys
      within_loan_data.each do |k,v|
        k = k.to_s if k.is_a? Symbol
        next unless k.in?(all_keys)
        v = v.to_f unless v.nil? || v.is_a?(Float)
        return_hash['missing_percentages'][k] += 1 if v.nil?
        return_hash['means'][k] += v unless v.nil?
      end
      (all_keys - within_loan_data.keys).each { |key| return_hash['missing_percentages'][key] += 1 }
    end
    return_hash['means'] = return_hash['means'].map { |k,v| { k => v / [1, (get_loans.count - return_hash['missing_percentages'][k])].max }}.reduce(:merge)
    return_hash['missing_percentages'] = return_hash['missing_percentages'].map { |k,v| {k => (v / get_loans.count rescue 0) }}.reduce(:merge)
    return_hash
  end    
end
